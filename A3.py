import sounddevice as sd
import numpy as np
import wave
import requests
import pygame
import time
import os
import webrtcvad
import threading
import keyboard
from aip import AipSpeech  # ä¿ç•™ï¼šä»…ç”¨äºç™¾åº¦ TTS
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog, scrolledtext

# âœ… æ–°å¢ï¼šWhisperï¼ˆfaster-whisperï¼‰
try:
    import torch

    _HAS_TORCH = True
except Exception:
    _HAS_TORCH = False
from faster_whisper import WhisperModel

# --------------------------- TTS é…ç½®ï¼ˆä¿ç•™ç™¾åº¦ TTSï¼Œä¸å˜ï¼‰ ---------------------------
TTS_CONFIG = {
    "app_id": "119812145",
    "api_key": "p9vdGZYrapwbhRNLVtBos2yi",
    "secret_key": "CQxpf1UWJlZEwKbUCfpgKryfbhtGU4hl",
    "default_out": "response.mp3",
    "per": 0,
    "vol": 5,
    "spd": 5,
    "pit": 5,
    "daily_char_limit": 999000
}
tts_client = AipSpeech(TTS_CONFIG["app_id"], TTS_CONFIG["api_key"], TTS_CONFIG["secret_key"])

CHAR_COUNT_FILE = "tts_char_count.txt"

# --------------------------- VAD é…ç½®ï¼ˆåˆ é™¤ä¸â€œä¸­æ–­æ’­æ”¾â€ç›¸å…³çš„å‚æ•°ï¼‰ ---------------------------
VAD_CONFIG = {
    "mode": 2,
    "sample_rate": 16000,
    "frame_duration_ms": 30,
    "silence_threshold": 30,
    # âŒ åˆ é™¤ï¼šä¸â€œä¸­æ–­æ’­æ”¾â€ç›¸å…³çš„å‚æ•°ï¼ˆåŸloud_sound_frames/min_loud_frames/max_loud_framesï¼‰
}
VAD_FRAME_SIZE = int(VAD_CONFIG["sample_rate"] * VAD_CONFIG["frame_duration_ms"] / 1000)

conversation_history = []
waiting_for_wakeup = True
DEFAULT_MIC = None
input_mode = "voice"
mode_lock = threading.Lock()
ai_background = ""
show_control_panel = False
wake_word = "ä½ å¥½"

# --------------------------- Whisper å…¨å±€åˆå§‹åŒ– ---------------------------
_WHISPER_MODEL = None


def _get_whisper_model():
    """å»¶è¿ŸåŠ è½½ Whisper æ¨¡å‹ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡"""
    global _WHISPER_MODEL
    if _WHISPER_MODEL is None:
        device = "cuda" if (_HAS_TORCH and torch.cuda.is_available()) else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"  # CPU ä¸Šç”¨ int8 èŠ‚çœå†…å­˜
        # æ¨¡å‹å¯æ”¹ï¼štiny / base / small / medium / large-v3
        _WHISPER_MODEL = WhisperModel("small", device=device, compute_type=compute_type)
        print(f"ğŸ§  Whisper å·²åŠ è½½ï¼šdevice={device}, compute_type={compute_type}")
    return _WHISPER_MODEL


def setup_mode_switch_listener():
    def on_enter_press():
        global input_mode
        with mode_lock:
            if input_mode != "text":
                input_mode = "text"
                print("\nâŒ¨ï¸ å·²åˆ‡æ¢åˆ°æ–‡å­—è¾“å…¥æ¨¡å¼ï¼ˆæŒ‰Tabé”®è¿”å›è¯­éŸ³è¾“å…¥ï¼‰")

    def on_tab_press():
        global input_mode
        with mode_lock:
            if input_mode != "voice":
                input_mode = "voice"
                print("\nğŸ¤ å·²åˆ‡æ¢åˆ°è¯­éŸ³è¾“å…¥æ¨¡å¼ï¼ˆæŒ‰Enteré”®åˆ‡æ¢åˆ°æ–‡å­—è¾“å…¥ï¼‰")

    def on_alt_press():
        global show_control_panel
        if not waiting_for_wakeup:
            show_control_panel = True
            print("\nâŒ¨ï¸ æ£€æµ‹åˆ°Alté”®ï¼Œå·²è®¾ç½®show_control_panel=True")

    keyboard.add_hotkey('enter', on_enter_press)
    keyboard.add_hotkey('tab', on_tab_press)
    keyboard.add_hotkey('alt', on_alt_press)
    print("âŒ¨ï¸ å¿«æ·é”®ï¼šæŒ‰Enteråˆ‡æ¢æ–‡å­—è¾“å…¥ï¼ŒTabåˆ‡æ¢è¯­éŸ³è¾“å…¥ï¼ŒAltæ‰“å¼€æ§åˆ¶é¢æ¿")


def get_text_input(prompt: str = "è¯·è¾“å…¥æ–‡å­—: ") -> str:
    root = tk.Tk()
    root.withdraw()
    user_input = simpledialog.askstring("æ–‡å­—è¾“å…¥", prompt)
    root.destroy()
    return user_input.strip() if user_input else ""


def create_control_panel():
    global ai_background, conversation_history, waiting_for_wakeup, wake_word

    root = tk.Tk()
    root.title("è¯­éŸ³åŠ©æ‰‹å‚æ•°æ§åˆ¶é¢æ¿")
    root.geometry("600x600")
    root.resizable(False, False)

    main_frame = tk.Frame(root)
    main_frame.pack(fill=tk.BOTH, expand=1)
    canvas = tk.Canvas(main_frame)
    scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=canvas.yview)
    scrollable_frame = tk.Frame(canvas)

    scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))

    canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
    canvas.configure(yscrollcommand=scrollbar.set)
    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)
    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)

    current_params = {
        "vad_mode": VAD_CONFIG["mode"],
        "vad_silence": VAD_CONFIG["silence_threshold"],
        # âŒ åˆ é™¤ï¼šåŸâ€œä¸­æ–­æ’­æ”¾â€ç›¸å…³çš„å‚æ•°ï¼ˆvad_loudï¼‰
        "tts_vol": TTS_CONFIG["vol"],
        "tts_spd": TTS_CONFIG["spd"],
        "tts_pit": TTS_CONFIG["pit"],
        "tts_per": TTS_CONFIG["per"],
        "ai_background": ai_background,
        "wake_word": wake_word
    }

    mbti_descriptions = {
        "INTJ": "ä½ æ˜¯ä¸€ä¸ªæˆ˜ç•¥æ€§ã€é€»è¾‘æ€§å¼ºçš„äººï¼Œæ“…é•¿é•¿æœŸè§„åˆ’ã€‚",
        "INTP": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢åˆ†æå’Œæ¢ç´¢æ¦‚å¿µçš„é€»è¾‘å‹æ€è€ƒè€…ã€‚",
        "ENTJ": "ä½ æ˜¯ä¸€ä¸ªæœæ–­ã€æ“…é•¿é¢†å¯¼å’Œç»„ç»‡çš„å¤–å‘å‹äººç‰©ã€‚",
        "ENTP": "ä½ æ˜¯ä¸€ä¸ªæœºæ™ºã€å–œæ¬¢è¾©è®ºå’Œæ–°æƒ³æ³•çš„æ¢ç´¢è€…ã€‚",
        "INFJ": "ä½ æ˜¯ä¸€ä¸ªç†æƒ³ä¸»ä¹‰è€…ï¼Œå…³å¿ƒä»–äººå¹¶å¯Œæœ‰æ´å¯ŸåŠ›ã€‚",
        "INFP": "ä½ æ˜¯ä¸€ä¸ªå†…çœã€å¯Œæœ‰åŒç†å¿ƒå’Œæƒ³è±¡åŠ›çš„äººã€‚",
        "ENFJ": "ä½ æ˜¯ä¸€ä¸ªå–„äºæ¿€åŠ±ä»–äººçš„é¢†å¯¼è€…å’Œæ²Ÿé€šè€…ã€‚",
        "ENFP": "ä½ æ˜¯ä¸€ä¸ªå¤–å‘ã€å¯Œæœ‰åˆ›é€ åŠ›å’ŒåŒç†å¿ƒçš„äººï¼Œå–œæ¬¢æ¢ç´¢æ–°æƒ³æ³•ã€‚",
        "ISTJ": "ä½ æ˜¯ä¸€ä¸ªå¯é ã€è´Ÿè´£ã€æ³¨é‡ç»†èŠ‚çš„ç°å®ä¸»ä¹‰è€…ã€‚",
        "ISFJ": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººã€é‡è§†å’Œè°çš„å®ˆæŠ¤è€…ã€‚",
        "ESTJ": "ä½ æ˜¯ä¸€ä¸ªåŠ¡å®ã€å–„äºç®¡ç†å’Œç»„ç»‡çš„é¢†å¯¼è€…ã€‚",
        "ESFJ": "ä½ æ˜¯ä¸€ä¸ªå–„äºåˆä½œã€å…³å¿ƒä»–äººçš„ç¤¾äº¤å‹äººç‰©ã€‚",
        "ISTP": "ä½ æ˜¯ä¸€ä¸ªåŠ¨æ‰‹èƒ½åŠ›å¼ºã€å–œæ¬¢æ¢ç´¢å®é™…è§£å†³æ–¹æ¡ˆçš„äººã€‚",
        "ISFP": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ã€å¯Œæœ‰åˆ›é€ åŠ›å’Œè‰ºæœ¯æ„Ÿçš„äººã€‚",
        "ESTP": "ä½ æ˜¯ä¸€ä¸ªè¡ŒåŠ¨å¯¼å‘ã€å–œæ¬¢å†’é™©å’Œå°è¯•æ–°äº‹ç‰©çš„äººã€‚",
        "ESFP": "ä½ æ˜¯ä¸€ä¸ªå¤–å‘ã€å–œæ¬¢è¡¨æ¼”å’Œäº«å—ç”Ÿæ´»çš„ä¹è§‚è€…ã€‚"
    }

    def save_params():
        nonlocal root
        global ai_background, wake_word, waiting_for_wakeup, conversation_history

        VAD_CONFIG["mode"] = int(vad_mode_slider.get())
        VAD_CONFIG["silence_threshold"] = int(vad_silence_slider.get())
        # âŒ åˆ é™¤ï¼šåŸâ€œä¸­æ–­æ’­æ”¾â€å‚æ•°çš„ä¿å­˜é€»è¾‘ï¼ˆvad_loudï¼‰
        TTS_CONFIG["vol"] = int(tts_vol_slider.get())
        TTS_CONFIG["spd"] = int(tts_spd_slider.get())
        TTS_CONFIG["pit"] = int(tts_pit_slider.get())
        TTS_CONFIG["per"] = tts_per_var.get()

        new_background = ai_background_text.get("1.0", tk.END).strip()
        new_wake_word = wake_word_entry.get().strip()
        selected_mbti = mbti_var.get()

        background_changed = new_background != current_params["ai_background"]
        wake_word_changed = new_wake_word != current_params["wake_word"] and new_wake_word

        if wake_word_changed:
            wake_word = new_wake_word

        if selected_mbti:
            ai_background = mbti_descriptions[selected_mbti] + "\n" + new_background
            conversation_history.clear()
            waiting_for_wakeup = True
        elif background_changed:
            ai_background = new_background
            conversation_history.clear()
            if not waiting_for_wakeup:
                waiting_for_wakeup = True

        messages = []
        if selected_mbti:
            messages.append(f"äººæ ¼å·²è®¾å®šä¸ºï¼š{selected_mbti}")
        if background_changed:
            messages.append("AIèƒŒæ™¯è®¾å®šå·²æ›´æ–°ï¼Œå¯¹è¯å·²é‡ç½®")
        if wake_word_changed:
            messages.append(f"å”¤é†’è¯å·²æ›´æ–°ä¸ºï¼š{new_wake_word}")
        if not messages:
            messages.append("å‚æ•°å·²ä¿å­˜")

        messagebox.showinfo("æˆåŠŸ", "\n".join(messages))
        root.destroy()

    def cancel_params():
        root.destroy()

    wake_word_frame = ttk.LabelFrame(scrollable_frame, text="å”¤é†’è¯è®¾ç½®", padding=(10, 5))
    wake_word_frame.pack(fill=tk.X, padx=20, pady=(15, 5))
    ttk.Label(wake_word_frame, text="å½“å‰å”¤é†’è¯ï¼š").grid(row=0, column=0, sticky=tk.W, pady=8, padx=(0, 10))
    wake_word_entry = ttk.Entry(wake_word_frame, width=30)
    wake_word_entry.grid(row=0, column=1, sticky=tk.W, pady=8)
    wake_word_entry.insert(0, current_params["wake_word"])

    ai_frame = ttk.LabelFrame(scrollable_frame, text="AIèƒŒæ™¯è®¾å®š", padding=(10, 5))
    ai_frame.pack(fill=tk.X, padx=20, pady=(10, 5))
    ai_background_text = scrolledtext.ScrolledText(ai_frame, height=5, wrap=tk.WORD)
    ai_background_text.pack(fill=tk.X, pady=(0, 10))
    ai_background_text.insert(tk.END, current_params["ai_background"])

    mbti_frame = ttk.LabelFrame(scrollable_frame, text="MBTIäººæ ¼è®¾å®š", padding=(10, 5))
    mbti_frame.pack(fill=tk.X, padx=20, pady=(10, 5))
    ttk.Label(mbti_frame, text="è¯·é€‰æ‹©äººæ ¼ç±»å‹ï¼š").grid(row=0, column=0, sticky=tk.W, pady=8)
    mbti_var = tk.StringVar(value="")
    mbti_menu = ttk.OptionMenu(mbti_frame, mbti_var, "", *mbti_descriptions.keys())
    mbti_menu.grid(row=0, column=1, padx=10, sticky=tk.EW)

    vad_frame = ttk.LabelFrame(scrollable_frame, text="VADè¯­éŸ³æ£€æµ‹é…ç½®", padding=(10, 5))
    vad_frame.pack(fill=tk.X, padx=20, pady=(10, 5))
    ttk.Label(vad_frame, text="VADçµæ•åº¦ï¼ˆ0=ä½ï¼Œ3=é«˜ï¼‰ï¼š").grid(row=0, column=0, sticky=tk.W, pady=8)
    vad_mode_slider = ttk.Scale(vad_frame, from_=0, to=3, orient=tk.HORIZONTAL, value=current_params["vad_mode"])
    vad_mode_slider.grid(row=0, column=1, padx=10, sticky=tk.EW)

    ttk.Label(vad_frame, text="å½•éŸ³é™éŸ³åœæ­¢æ—¶é—´ï¼ˆå¸§ï¼Œ1å¸§=30msï¼‰ï¼š").grid(row=1, column=0, sticky=tk.W, pady=8)
    vad_silence_slider = ttk.Scale(vad_frame, from_=50, to=200, orient=tk.HORIZONTAL,
                                   value=current_params["vad_silence"])
    vad_silence_slider.grid(row=1, column=1, padx=10, sticky=tk.EW)

    # âŒ åˆ é™¤ï¼šæ§åˆ¶é¢æ¿ä¸­â€œä¸­æ–­æ’­æ”¾å£°éŸ³æ—¶é•¿â€çš„æ»‘å—ï¼ˆåŸrow=2çš„vad_loudç›¸å…³æ§ä»¶ï¼‰

    tts_frame = ttk.LabelFrame(scrollable_frame, text="TTSè¯­éŸ³åˆæˆé…ç½®", padding=(10, 5))
    tts_frame.pack(fill=tk.X, padx=20, pady=(10, 5))
    ttk.Label(tts_frame, text="éŸ³é‡ï¼ˆ0-15ï¼‰ï¼š").grid(row=0, column=0, sticky=tk.W, pady=8)
    tts_vol_slider = ttk.Scale(tts_frame, from_=0, to=15, orient=tk.HORIZONTAL, value=current_params["tts_vol"])
    tts_vol_slider.grid(row=0, column=1, padx=10, sticky=tk.EW)

    ttk.Label(tts_frame, text="è¯­é€Ÿï¼ˆ0-9ï¼‰ï¼š").grid(row=1, column=0, sticky=tk.W, pady=8)
    tts_spd_slider = ttk.Scale(tts_frame, from_=0, to=9, orient=tk.HORIZONTAL, value=current_params["tts_spd"])
    tts_spd_slider.grid(row=1, column=1, padx=10, sticky=tk.EW)

    ttk.Label(tts_frame, text="è¯­è°ƒï¼ˆ0-9ï¼‰ï¼š").grid(row=2, column=0, sticky=tk.W, pady=8)
    tts_pit_slider = ttk.Scale(tts_frame, from_=0, to=9, orient=tk.HORIZONTAL, value=current_params["tts_pit"])
    tts_pit_slider.grid(row=2, column=1, padx=10, sticky=tk.EW)

    tts_per_options = {0: "åº¦å°é¹¿", 1: "åº¦åšæ–‡", 3: "åº¦é€é¥", 4: "åº¦ä¸«ä¸«", 5: "åº¦å°å¨‡"}
    ttk.Label(tts_frame, text="å‘éŸ³äººï¼š").grid(row=3, column=0, sticky=tk.W, pady=8)
    tts_per_var = tk.IntVar(value=current_params["tts_per"])
    tts_per_menu = ttk.OptionMenu(tts_frame, tts_per_var, current_params["tts_per"], *tts_per_options.keys())
    tts_per_menu.grid(row=3, column=1, padx=10, sticky=tk.EW)

    btn_frame = ttk.Frame(scrollable_frame, padding=(10, 5))
    btn_frame.pack(fill=tk.X, padx=20, pady=(15, 20))
    save_btn = ttk.Button(btn_frame, text="ä¿å­˜å‚æ•°", command=save_params)
    save_btn.pack(side=tk.LEFT, padx=20, fill=tk.X, expand=True)
    cancel_btn = ttk.Button(btn_frame, text="å–æ¶ˆï¼ˆä¸ä¿å­˜ï¼‰", command=cancel_params)
    cancel_btn.pack(side=tk.RIGHT, padx=20, fill=tk.X, expand=True)

    root.mainloop()


# --------------------------- éº¦å…‹é£è®¾å¤‡è·å–é€»è¾‘ï¼ˆåŸæ ·ä¿ç•™ï¼‰ ---------------------------
def get_default_microphone() -> int | None:
    """å¥å£®è·å–éº¦å…‹é£è®¾å¤‡ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©ä¸å…¼å®¹æ¨¡å¼"""
    try:
        print("ğŸ” æ­£åœ¨æ£€æµ‹éº¦å…‹é£è®¾å¤‡...")
        all_devices = sd.query_devices()
        input_devices = [dev for dev in all_devices if dev['max_input_channels'] > 0]

        if input_devices:
            print("\nğŸ“‹ æ£€æµ‹åˆ°ä»¥ä¸‹éº¦å…‹é£è®¾å¤‡ï¼š")
            for idx, dev in enumerate(input_devices):
                print(
                    f"   {idx + 1}. è®¾å¤‡åï¼š{dev['name']} | è¾“å…¥å£°é“ï¼š{dev['max_input_channels']} | è®¾å¤‡IDï¼š{dev['index']}")
            default_mic_id = input_devices[0]['index']
            print(f"\nğŸ¤ è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªéº¦å…‹é£ï¼š{input_devices[0]['name']}ï¼ˆè®¾å¤‡IDï¼š{default_mic_id}ï¼‰")
            return default_mic_id
        else:
            print("âš ï¸  æœªæŸ¥è¯¢åˆ°æ˜ç¡®çš„è¾“å…¥è®¾å¤‡ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ¨¡å¼")
            default_input = sd.default.device[0]
            default_dev_info = sd.query_devices(default_input)
            if default_dev_info['max_input_channels'] > 0:
                print(f"ğŸ¤ ä½¿ç”¨ç³»ç»Ÿé»˜è®¤éº¦å…‹é£ï¼ˆè®¾å¤‡IDï¼š{default_input}ï¼Œåç§°ï¼š{default_dev_info['name']}ï¼‰")
                return default_input
            else:
                print("âš ï¸  ç³»ç»Ÿé»˜è®¤è®¾å¤‡ééº¦å…‹é£ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨é€‚é…æ¨¡å¼")
                return None
    except Exception as e:
        print(f"\nâŒ è®¾å¤‡æŸ¥è¯¢å‡ºé”™ï¼š{str(e)}")
        print("ğŸ’¡ å·²åˆ‡æ¢åˆ°å…¼å®¹æ¨¡å¼ï¼Œå°è¯•è‡ªåŠ¨é€‚é…éº¦å…‹é£")
        return None


# åˆå§‹åŒ–éº¦å…‹é£
DEFAULT_MIC = get_default_microphone()


# --------------------------- 2. é…é¢ç®¡ç†å·¥å…·ï¼ˆåŸæ ·ä¿ç•™ï¼‰ ---------------------------
def get_daily_char_usage() -> int:
    if not os.path.exists(CHAR_COUNT_FILE):
        return 0
    today = time.strftime("%Y-%m-%d")
    with open(CHAR_COUNT_FILE, "r") as f:
        lines = f.readlines()
        if len(lines) >= 2 and lines[0].strip() == today:
            return int(lines[1].strip())
    return 0


def update_char_usage(added_chars: int) -> None:
    today = time.strftime("%Y-%m-%d")
    current = get_daily_char_usage()
    new_total = current + added_chars
    with open(CHAR_COUNT_FILE, "w") as f:
        f.write(f"{today}\n{new_total}\n")
    print(f"ğŸ“Š TTSå­—ç¬¦ä½¿ç”¨ï¼šä»Šæ—¥å·²ç”¨ {new_total}/{TTS_CONFIG['daily_char_limit']}")


def check_quota(text: str) -> bool:
    text_len = len(text)
    used = get_daily_char_usage()
    if used + text_len > TTS_CONFIG["daily_char_limit"]:
        print(f"âŒ é…é¢ä¸è¶³ï¼ä»Šæ—¥å·²ç”¨{used}å­—ç¬¦ï¼Œè¿˜éœ€{text_len}å­—ç¬¦ï¼ˆä¸Šé™{TTS_CONFIG['daily_char_limit']}ï¼‰")
        return False
    return True


# --------------------------- 3. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆåˆ é™¤â€œè¯­éŸ³ä¸­æ–­æ’­æ”¾â€é€»è¾‘ï¼‰ ---------------------------
def record_wav_16k(out_wav="recorded.wav") -> str | None:
    """å½•éŸ³å‡½æ•°ï¼ˆ16kHzå•å£°é“ï¼‰ï¼Œæ”¯æŒè®¾å¤‡å…¼å®¹ä¸é‡è¯•æœºåˆ¶"""
    vad = webrtcvad.Vad(VAD_CONFIG["mode"])
    RATE, CHANNELS, WIDTH = VAD_CONFIG["sample_rate"], 1, 2
    print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼ˆåœæ­¢è¯´è¯æŒ‡å®šæ—¶é—´åè‡ªåŠ¨ç»“æŸï¼‰...")

    audio_chunks = []
    silence_frame_count = 0
    max_retry = 3
    retry_count = 0

    while retry_count < max_retry:
        try:
            stream_kwargs = {
                "samplerate": RATE,
                "channels": CHANNELS,
                "dtype": np.int16
            }
            if DEFAULT_MIC is not None:
                stream_kwargs["device"] = DEFAULT_MIC

            with sd.InputStream(**stream_kwargs) as stream:
                while True:
                    chunk, overflow = stream.read(VAD_FRAME_SIZE)
                    if overflow:
                        print("âš ï¸  å½•éŸ³ç¼“å†²åŒºæº¢å‡ºï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½ä¸¢å¤±")

                    chunk_bytes = chunk.tobytes()
                    is_speech = vad.is_speech(chunk_bytes, RATE)

                    if is_speech:
                        audio_chunks.append(chunk)
                        silence_frame_count = 0
                        print(f"ğŸ”Š æ£€æµ‹åˆ°è¯­éŸ³... é™éŸ³è®¡æ•°: {silence_frame_count}/{VAD_CONFIG['silence_threshold']}",
                              end="\r")
                    else:
                        if audio_chunks:
                            audio_chunks.append(chunk)
                            silence_frame_count += 1
                            silence_sec = silence_frame_count * 0.03
                            print(
                                f"ğŸŸ¡ é™éŸ³ä¸­... è®¡æ•°: {silence_frame_count}/{VAD_CONFIG['silence_threshold']}ï¼ˆ{silence_sec:.1f}ç§’ååœæ­¢ï¼‰",
                                end="\r")
                            if silence_frame_count >= VAD_CONFIG["silence_threshold"]:
                                print(f"\nğŸ›‘ æ£€æµ‹åˆ°{silence_sec:.1f}ç§’é™éŸ³ï¼Œåœæ­¢å½•éŸ³")
                                break
                        else:
                            print("ğŸŸ¡ ç­‰å¾…è¯­éŸ³è¾“å…¥...", end="\r")
            break
        except sd.PortAudioError as e:
            retry_count += 1
            error_msg = str(e).lower()
            if "host error" in error_msg or "mme error" in error_msg:
                print(f"\nâŒ å½•éŸ³å¤±è´¥ï¼ˆ{retry_count}/{max_retry}ï¼‰ï¼šéº¦å…‹é£è¢«å ç”¨æˆ–é©±åŠ¨é”™è¯¯")
                print("ğŸ’¡ å»ºè®®ï¼šå…³é—­å¾®ä¿¡ã€QQç­‰å ç”¨éº¦å…‹é£çš„ç¨‹åº")
            else:
                print(f"\nâŒ å½•éŸ³å¤±è´¥ï¼ˆ{retry_count}/{max_retry}ï¼‰ï¼š{str(e)}")
            if retry_count < max_retry:
                print("ğŸ”„ 1ç§’åé‡è¯•...")
                time.sleep(1)
        except Exception as e:
            print(f"\nâŒ å½•éŸ³å¼‚å¸¸ï¼š{str(e)}")
            return None

    if retry_count >= max_retry:
        print("âŒ å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œå¯èƒ½éº¦å…‹é£è¢«å ç”¨æˆ–é©±åŠ¨å¼‚å¸¸")
        return None

    if not audio_chunks:
        print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œå½•éŸ³å–æ¶ˆ")
        return None

    audio_data = np.concatenate(audio_chunks, axis=0)
    with wave.open(out_wav, "wb") as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(WIDTH)
        wf.setframerate(RATE)
        wf.writeframes(audio_data.tobytes())

    print(f"âœ… å½•éŸ³ä¿å­˜è‡³ï¼š{out_wav}")
    return out_wav


def play_audio_with_interrupt(file_path: str) -> None:
    """æ’­æ”¾éŸ³é¢‘ï¼ˆâŒ å·²åˆ é™¤â€œå¬åˆ°è¯­éŸ³ä¸­æ–­â€é€»è¾‘ï¼Œä»…ä¿ç•™åŸºç¡€æ’­æ”¾ï¼‰"""
    if not os.path.exists(file_path):
        print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return

    pygame.mixer.init()
    try:
        pygame.mixer.music.load(file_path)
        # âŒ åˆ é™¤åŸâ€œæ£€æµ‹å£°éŸ³å¯ä¸­æ–­â€çš„æç¤ºï¼Œæ”¹ä¸ºåŸºç¡€æ’­æ”¾æç¤º
        print(f"ğŸ”Š å¼€å§‹æ’­æ”¾ï¼š{file_path}")
        pygame.mixer.music.play()

        # âŒ å®Œå…¨åˆ é™¤ï¼šåŸâ€œç›‘å¬éº¦å…‹é£å£°éŸ³å¹¶ä¸­æ–­æ’­æ”¾â€çš„æ ¸å¿ƒé€»è¾‘ï¼ˆåŒ…æ‹¬VADåˆå§‹åŒ–ã€æµç›‘å¬ã€ä¸­æ–­åˆ¤æ–­ï¼‰

        # âœ… ä¿ç•™ï¼šä»…ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ
        print("ğŸŸ¡ æ’­æ”¾ä¸­...", end="\r")
        while pygame.mixer.music.get_busy():
            time.sleep(0.1)

        print("âœ… æ’­æ”¾ç»“æŸ")
    except pygame.error as e:
        print(f"âŒ æ’­æ”¾å¤±è´¥ï¼š{str(e)}")
    finally:
        pygame.mixer.quit()


# --------------------------- 4. æ ¸å¿ƒåŠŸèƒ½ï¼ˆè¿™é‡Œæ›¿æ¢ä¸º Whisperï¼‰ ---------------------------
def speech_to_text(wav_path: str) -> str | None:
    """è¯­éŸ³è½¬æ–‡å­—ï¼ˆWhisper/faster-whisper å®ç°ï¼Œæ›¿ä»£ç™¾åº¦ ASRï¼‰"""
    if not wav_path or not os.path.exists(wav_path):
        print("âŒ æ— æ•ˆçš„éŸ³é¢‘è·¯å¾„")
        return None

    try:
        model = _get_whisper_model()
        # è¯´æ˜ï¼š
        # - language="zh"ï¼šä¸­æ–‡è¯†åˆ«æ›´ç¨³å®šï¼›å¯ç½® None è®©æ¨¡å‹è‡ªåŠ¨æ£€æµ‹
        # - vad_filter=Trueï¼šå¯¹é•¿éŸ³é¢‘åšæ›´ç¨³çš„åˆ†æ®µ
        # - beam_size=5ï¼šæé«˜å‡†ç¡®ç‡ï¼ˆé€Ÿåº¦ç•¥æ…¢ï¼‰
        segments, info = model.transcribe(
            wav_path,
            language="zh",
            vad_filter=True,
            beam_size=5,
            word_timestamps=False
        )
        text = "".join(seg.text for seg in segments).strip()
        if text:
            print(f"âœ… Whisperè¯†åˆ«ç»“æœï¼š{text}")
            return text
        print("âŒ Whisper æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
        return None
    except Exception as e:
        print(f"âŒ Whisper è¯†åˆ«å¤±è´¥ï¼š{str(e)}")
        return None


def text_to_speech(text: str, out_file: str = TTS_CONFIG["default_out"]) -> str | None:
    """æ–‡å­—è½¬è¯­éŸ³ï¼ˆç™¾åº¦ TTSï¼Œä¿ç•™ä¸å˜ï¼‰"""
    for char in ['#', '&', '@', '!', '*', ';']:
        text = text.replace(char, '')
    max_len = min(120, TTS_CONFIG["daily_char_limit"] - get_daily_char_usage())
    if len(text) > max_len:
        text = text[:max_len - 3] + "..."
        print(f"âš ï¸  æ–‡æœ¬æˆªæ–­ï¼š{text}")
    if not text:
        print("âŒ æ— æœ‰æ•ˆæ–‡æœ¬")
        return None
    if not check_quota(text):
        return None

    try:
        result = tts_client.synthesis(
            text, lang="zh", ctp=1,
            options={
                "vol": TTS_CONFIG["vol"],
                "spd": TTS_CONFIG["spd"],
                "pit": TTS_CONFIG["pit"],
                "per": TTS_CONFIG["per"]
            }
        )
        if isinstance(result, dict):
            print(f"âŒ TTSå¤±è´¥ï¼š{result.get('err_msg')}")
            if "limit reached" in result.get("err_msg", "").lower():
                print("ğŸ’¡ éœ€è´­ä¹°èµ„æºåŒ…æˆ–æ¬¡æ—¥é‡ç½®é…é¢")
            return None
        with open(out_file, "wb") as f:
            f.write(result)
        update_char_usage(len(text))
        print(f"âœ… TTSä¿å­˜è‡³ï¼š{out_file} | æ–‡æœ¬ï¼š{text}")
        return out_file
    except Exception as e:
        print(f"âŒ TTSè¯·æ±‚å¤±è´¥ï¼š{str(e)}")
        return None


def call_chat_api(prompt: str) -> str | None:
    """è°ƒç”¨æœ¬åœ°èŠå¤©APIï¼Œæ”¯æŒå¯¹è¯å†å²è®°å¿†å’ŒAIèƒŒæ™¯è®¾å®š"""
    global conversation_history, ai_background

    if not prompt:
        print("âŒ æ— å¯¹è¯å†…å®¹")
        return None

    system_prompt = ""
    if ai_background:
        system_prompt = f"ä»¥ä¸‹æ˜¯ä½ çš„èƒŒæ™¯è®¾å®šï¼Œè¯·ä¸¥æ ¼éµå®ˆï¼š{ai_background}\n\n"

    full_conversation = "\n".join([
        f"ç”¨æˆ·: {item['user']}\nAI: {item['ai']}"
        for item in conversation_history
    ])

    if full_conversation:
        full_prompt = f"{system_prompt}{full_conversation}\nç”¨æˆ·: {prompt}\nAI:"
    else:
        full_prompt = f"{system_prompt}ç”¨æˆ·: {prompt}\nAI:"

    url = "http://localhost:8000/chat/"
    data = {
        "prompt": full_prompt,
        "options": {"temperature": 0.7, "max_tokens": 200}
    }

    try:
        response = requests.post(url, json=data, timeout=20)
        response.raise_for_status()
        reply = response.json().get("response", "").strip()
        print(f"âœ… Chatå›å¤ï¼š{reply}")
        return reply
    except Exception as e:
        print(f"âŒ Chatè¯·æ±‚å¤±è´¥ï¼š{str(e)}")
        return None


def test_health_api() -> None:
    url = "http://localhost:8000/health"
    try:
        response = requests.get(url, timeout=5)
        print(f"âœ… å¥åº·æ£€æŸ¥ï¼š{response.json()}")
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼š{str(e)}")


# --------------------------- 5. ä¸šåŠ¡æµç¨‹ï¼ˆåˆ é™¤â€œä¸­æ–­æ’­æ”¾â€ç›¸å…³æç¤ºï¼‰ ---------------------------
def wake_up_detect() -> bool:
    """å”¤é†’æ£€æµ‹ï¼šä½¿ç”¨å¯é…ç½®çš„å”¤é†’è¯"""
    global conversation_history, waiting_for_wakeup, input_mode, wake_word

    if waiting_for_wakeup:
        with mode_lock:
            input_mode = "voice"
        conversation_history = []
        print("ğŸ§¹ å·²æ¸…ç©ºå†å²è®°å½•ï¼Œç­‰å¾…æ–°çš„å”¤é†’...")

    print(f"\nğŸ” ç­‰å¾…å”¤é†’è¯ï¼ˆ{wake_word}ï¼‰ï¼Œåœæ­¢è¯´è¯æŒ‡å®šæ—¶é—´åè‡ªåŠ¨å½•éŸ³ç»“æŸ...")
    wav_file = record_wav_16k(out_wav="wake_up.wav")
    if not wav_file:
        return False

    recognized_text = speech_to_text(wav_file)
    if recognized_text and wake_word in recognized_text:
        if ai_background:
            print(f"âœ… æ£€æµ‹åˆ°å”¤é†’è¯ï¼š{wake_word}ï¼ˆå½“å‰AIèƒŒæ™¯è®¾å®šå·²ç”Ÿæ•ˆï¼‰")
        else:
            print(f"âœ… æ£€æµ‹åˆ°å”¤é†’è¯ï¼š{wake_word}")
        waiting_for_wakeup = False
        return True

    print(f"âŒ æœªæ£€æµ‹åˆ°å”¤é†’è¯ï¼ˆè¯†åˆ«ç»“æœï¼š{recognized_text or 'æ— '}ï¼Œå”¤é†’è¯ï¼š{wake_word}ï¼‰")
    return False


def voice_interaction_flow() -> None:
    """å®Œæ•´è¯­éŸ³äº¤äº’æµç¨‹ï¼ˆâŒ åˆ é™¤â€œä¸­æ–­æ’­æ”¾â€ç›¸å…³æç¤ºï¼‰"""
    global waiting_for_wakeup, input_mode, conversation_history, ai_background, show_control_panel, wake_word

    print("=" * 50)
    print(f"ğŸ¯ è¯­éŸ³åŠ©æ‰‹å¯åŠ¨ï¼šå½“å‰å”¤é†’è¯ä¸ºã€Œ{wake_word}ã€ï¼ˆå¯åœ¨æ§åˆ¶é¢æ¿ä¿®æ”¹ï¼‰")
    print(f"ğŸ¯ TTSæ¯æ—¥é…é¢ï¼š{TTS_CONFIG['daily_char_limit']}å­—")
    print("=" * 50)

    setup_mode_switch_listener()

    if DEFAULT_MIC is None:
        print("âš ï¸  æœªæ˜ç¡®æŒ‡å®šéº¦å…‹é£è®¾å¤‡ï¼Œå°†å°è¯•è‡ªåŠ¨é€‚é…ï¼ˆå¯èƒ½å½±å“å½•éŸ³ç¨³å®šæ€§ï¼‰")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥éº¦å…‹é£é©±åŠ¨æˆ–å…³é—­å ç”¨éº¦å…‹é£çš„ç¨‹åº\n")

    while True:
        while not wake_up_detect():
            time.sleep(0.5)

        print("\nğŸ‰ å”¤é†’æˆåŠŸï¼æ”¯æŒæŒ‡ä»¤ï¼š")
        print(f"   - å”¤é†’è¯ï¼šå½“å‰ä¸ºã€Œ{wake_word}ã€ï¼ˆå¯åœ¨æ§åˆ¶é¢æ¿ä¿®æ”¹ï¼‰")
        print("   - æ­£å¸¸å¯¹è¯ï¼šç›´æ¥è¯´è¯æé—®")
        print("   - æ‰“å¼€æ§åˆ¶é¢æ¿ï¼šè¯´ã€æ§åˆ¶é¢æ¿ã€‘æˆ–æŒ‰Alté”®")
        print("   - è¿”å›å”¤é†’ï¼šè¯´ã€å†è§ã€‘")
        print("   - åˆ‡æ¢è¾“å…¥æ¨¡å¼ï¼šæŒ‰Enteré”®(æ–‡å­—)/Tabé”®(è¯­éŸ³)")
        print(f"â„¹ï¸ TTSæ¯æ—¥é…é¢ï¼š{get_daily_char_usage()}/{TTS_CONFIG['daily_char_limit']}å­—")

        if ai_background:
            print(f"â„¹ï¸ å½“å‰AIèƒŒæ™¯è®¾å®šï¼š{ai_background[:50]}{'...' if len(ai_background) > 50 else ''}")

        print("ğŸ’¡ æç¤ºï¼šå½“å‰è¾“å…¥æ¨¡å¼ - " + (
            "æ–‡å­—è¾“å…¥ï¼ˆæŒ‰Tabè¿”å›è¯­éŸ³ï¼‰" if input_mode == "text" else "è¯­éŸ³è¾“å…¥ï¼ˆæŒ‰Enteråˆ‡æ¢æ–‡å­—ï¼‰\n"))

        while not waiting_for_wakeup:
            if show_control_panel:
                show_control_panel = False
                print(f"ğŸ”§ å‡†å¤‡æ‰“å¼€æ§åˆ¶é¢æ¿...")
                panel_audio = text_to_speech("æ­£åœ¨ä¸ºæ‚¨æ‰“å¼€å‚æ•°æ§åˆ¶é¢æ¿ï¼Œè¯·åœ¨çª—å£ä¸­è°ƒèŠ‚å‚æ•°ã€‚", "panel_notify.mp3")
                if panel_audio:
                    play_audio_with_interrupt(panel_audio)
                create_control_panel()

                print(f"â„¹ï¸ å½“å‰å”¤é†’è¯å·²æ›´æ–°ä¸ºï¼šã€Œ{wake_word}ã€")

                if waiting_for_wakeup:
                    wakeup_again_audio = text_to_speech(f"AIå·²é‡ç½®ï¼Œè¯·è¯´{wake_word}é‡æ–°å”¤é†’ã€‚", "wakeup_again.mp3")
                    if wakeup_again_audio:
                        play_audio_with_interrupt(wakeup_again_audio)
                    break

                after_panel_audio = text_to_speech("å‚æ•°é¢æ¿å·²å…³é—­ï¼Œå¯ç»§ç»­æ­£å¸¸å¯¹è¯ã€‚", "after_panel.mp3")
                if after_panel_audio:
                    play_audio_with_interrupt(after_panel_audio)
                continue

            current_mode = input_mode

            if current_mode == "text":
                user_text = get_text_input("è¯·è¾“å…¥æ–‡å­—ï¼ˆæŒ‰Cancelè¿”å›è¯­éŸ³è¾“å…¥ï¼‰: ")
                if not user_text:
                    with mode_lock:
                        input_mode = "voice"
                    print("ğŸ¤ å·²åˆ‡æ¢åˆ°è¯­éŸ³è¾“å…¥æ¨¡å¼")
                    continue
                print(f"ğŸ’¬ ç”¨æˆ·ï¼ˆæ–‡å­—è¾“å…¥ï¼‰ï¼š{user_text}")
            else:
                user_audio = record_wav_16k("user_command.wav")
                if not user_audio:
                    continue

                user_text = speech_to_text(user_audio)
                if not user_text:
                    retry_audio = text_to_speech("æˆ‘æ²¡å¬æ¸…ï¼Œè¯·å†è¯´ä¸€é~", "retry.mp3")
                    if retry_audio:
                        play_audio_with_interrupt(retry_audio)
                    continue

            if "æ§åˆ¶é¢æ¿" in user_text:
                print(f"ğŸ”§ æ”¶åˆ°æŒ‡ä»¤ï¼š{user_text} â†’ å¯åŠ¨å‚æ•°é¢æ¿")
                panel_audio = text_to_speech("æ­£åœ¨ä¸ºæ‚¨æ‰“å¼€å‚æ•°æ§åˆ¶é¢æ¿ï¼Œè¯·åœ¨çª—å£ä¸­è°ƒèŠ‚å‚æ•°ã€‚", "panel_notify.mp3")
                if panel_audio:
                    play_audio_with_interrupt(panel_audio)
                create_control_panel()

                print(f"â„¹ï¸ å½“å‰å”¤é†’è¯å·²æ›´æ–°ä¸ºï¼šã€Œ{wake_word}ã€")

                if waiting_for_wakeup:
                    wakeup_again_audio = text_to_speech(f"AIå·²é‡ç½®ï¼Œè¯·è¯´{wake_word}é‡æ–°å”¤é†’ã€‚", "wakeup_again.mp3")
                    if wakeup_again_audio:
                        play_audio_with_interrupt(wakeup_again_audio)
                    break

                after_panel_audio = text_to_speech("å‚æ•°é¢æ¿å·²å…³é—­ï¼Œå¯ç»§ç»­æ­£å¸¸å¯¹è¯ã€‚", "after_panel.mp3")
                if after_panel_audio:
                    play_audio_with_interrupt(after_panel_audio)
                continue

            elif "å†è§" in user_text:
                print(f"ğŸ‘‹ æ”¶åˆ°è¿”å›æŒ‡ä»¤ï¼š{user_text}")
                exit_audio = text_to_speech(f"å·²è¿”å›å”¤é†’ç­‰å¾…çŠ¶æ€ï¼Œè¯´{wake_word}ç»§ç»­äº¤æµã€‚", "exit.mp3")
                if exit_audio:
                    play_audio_with_interrupt(exit_audio)
                waiting_for_wakeup = True
                break

            else:
                chat_reply = call_chat_api(user_text)
                if not chat_reply:
                    no_reply_audio = text_to_speech("æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å›å¤ï¼Œè¯·ç¨åå†è¯•ã€‚", "no_reply.mp3")
                    if no_reply_audio:
                        play_audio_with_interrupt(no_reply_audio)
                    continue

                conversation_history.append({"user": user_text, "ai": chat_reply})
                if len(conversation_history) > 10:
                    conversation_history = conversation_history[-10:]
                print(f"ğŸ“ å·²ä¿å­˜å¯¹è¯å†å²ï¼ˆå…±{len(conversation_history)}è½®ï¼‰")

                print(f"ğŸ¤– AIï¼š{chat_reply}")
                reply_audio = text_to_speech(chat_reply, "chat_reply.mp3")
                if reply_audio:
                    play_audio_with_interrupt(reply_audio)

            print("\n" + "-" * 30)
            print(f"ğŸ’¡ å”¤é†’è¯ï¼šã€Œ{wake_word}ã€ï¼ˆå¯åœ¨æ§åˆ¶é¢æ¿ä¿®æ”¹ï¼‰")
            print("ğŸ’¡ å¯ç»§ç»­æ“ä½œï¼šè¯´é—®é¢˜/è¯´æ§åˆ¶é¢æ¿/æŒ‰Alté”®/å†è§")
            print(f"ğŸ’¡ å½“å‰è¾“å…¥æ¨¡å¼ï¼š{input_mode}ï¼ˆæŒ‰Enteråˆ‡æ¢æ–‡å­—ï¼ŒæŒ‰Tabåˆ‡æ¢è¯­éŸ³ï¼‰")
            print(
                f"ğŸ’¡ TTSé…é¢å‰©ä½™ï¼š{TTS_CONFIG['daily_char_limit'] - get_daily_char_usage()}/{TTS_CONFIG['daily_char_limit']}å­—")
            print("-" * 30 + "\n")


# --------------------------- 6. æ‰§è¡Œå…¥å£ ---------------------------
if __name__ == "__main__":
    test_health_api()
    try:
        voice_interaction_flow()
    except KeyboardInterrupt:
        print("\nğŸ”Œ ç¨‹åºè¢«æ‰‹åŠ¨ä¸­æ–­")
    finally:
        for tmp_file in ["wake_up.wav", "user_command.wav", "chat_reply.mp3", "retry.mp3",
                         "no_reply.mp3", "exit.mp3", "panel_notify.mp3", "after_panel.mp3", "wakeup_again.mp3"]:
            if os.path.exists(tmp_file):
                os.remove(tmp_file)
        print("ğŸ—‘ï¸  ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")